# KG full name (used in prompts)
kg_full_name: PubChem knowledge graph
#kg_full_name: Integrated Database of Small Molecules knowledge graph (IDSM)

# KG short name (used to generate file paths)
kg_short_name: idsm

# KG textual description (used in prompts)
kg_description: The IDSM SPARQL endpoint provides fast similarity and structural search functionality in knowledge graph such as ChEMBL, ChEBI or PubChem.

# SPARQL endpoint serving the KG
kg_sparql_endpoint_url: "https://idsm.elixir-czech.cz/sparql/endpoint/idsm" 

# Prefixes and namespaces to be used in the Turtle and SPARQL queries
prefixes:
  bao:            "http://www.bioassayontology.org/bao#"
  cito:           "http://purl.org/spar/cito/"
  chembl:         "http://rdf.ebi.ac.uk/terms/chembl#"
  dc:             "http://purl.org/dc/elements/1.1/"
  dcterms:        "http://purl.org/dc/terms/"
  enpkg:          "https://enpkg.commons-lab.org/kg/"
  enpkg_module:   "https://enpkg.commons-lab.org/module/"
  obo:            "http://purl.obolibrary.org/obo/"
  owl:            "http://www.w3.org/2002/07/owl#"
  pav:            "http://purl.org/pav/"
  pubchem:        "http://rdf.ncbi.nlm.nih.gov/pubchem/vocabulary#"
  rdf:            "http://www.w3.org/1999/02/22-rdf-syntax-ns#"
  rdfs:           "http://www.w3.org/2000/01/rdf-schema#"
  schema:         "http://schema.org/"
  snomedct:       "http://purl.bioontology.org/ontology/SNOMEDCT/"
  sio:            "http://semanticscience.org/resource/"
  xsd:            "http://www.w3.org/2001/XMLSchema#"

# Root path for the cache of classes context and pre-computed embeddings
data_directory: "./data"

# Format of the classes context: one of "turtle", "tuple"
class_context_format: tuple

# Prefix of the file names for the pre-computed embeddings of classes context. Will be appended with "{model_id}_{vector_db}_index"
# e.g. "v3_4_full_" will yield something like "v3_4_full_nomic-embed-text_faiss_index"
class_context_embeddings_prefix: "v3_4_full_" 

# Prefix of the file names for the pre-computed embeddings of SPARQL queries. Will be appended with "{model_id}_{vector_db}_index"
# e.g. "query_v1_" will yield something like "query_v1_nomic-embed-text_faiss_index"
queries_embeddings_prefix: "query_v1_" 

# Path to a usable temporary directory
temp_directory: "./tmp"

# embedding_generation:
  # Utilitary endpoint to store the ontologies (if not present in the endpoint to be queried) 
  # work_sparql_endpoint_url: ""
  # text_embedding_llm:
    # server_type: ollama-embeddings
    # id: mxbai-embed-large
    # vector_db: faiss
    # output_directory: "data/"

# -----------------------------------------------------------
# LLM configurations

seq2seq_models:

  llama3_2-1b@local:
    server_type: ollama
    id: llama3.2:1b
    temperature: 0
    max_retries: 3
    "top_p": 0.95
  
  llama-3_1-70B@ovh:
    server_type: ovh
    id: Meta-Llama-3_1-70B-Instruct
    base_url: https://llama-3-1-70b-instruct.endpoints.kepler.ai.cloud.ovh.net/api/openai_compat/v1
    temperature: 0
    max_retries: 3
    "top_p": 0.95

  gpt-4o@openai:
    server_type: openai
    id: gpt-4o
    temperature: 0
    max_retries: 3
    "top_p": 0.95
    
  o3-mini@openai:
    server_type: openai
    id: o3-mini
    
  deepseek-r1_1_5b@local:
    server_type: ollama
    id: deepseek-r1:1.5b
    temperature: 0
    max_retries: 3 
    "top_p": 0.95

  deepseek-chat@deepseek:
    server_type: deepseek
    id: deepseek-chat
    base_url: https://api.deepseek.com
    temperature: 0
    max_retries: 3
    "top_p": 0.95

  deepseek-reasoner@deepseek:
    server_type: deepseek
    id: deepseek-reasoner
    base_url: https://api.deepseek.com
    temperature: 0
    max_retries: 3
    "top_p": 0.95

  deepseek-reasoner@deepseek:
    server_type: hugface
    id: deepseek-ai/DeepSeek-R1-Distill-Qwen-32B

text_embedding_models:

  nomic-embed-text_faiss@local:
    server_type: ollama-embeddings
    id: nomic-embed-text
    vector_db: faiss

  nomic-embed-text_chroma@local:
    server_type: openai-embeddings
    id: nomic-embed-text
    vector_db: chroma
  
# -----------------------------------------------------------
# Scenarios
      
scenario_1:
  seq2seq_model: llama3_2-1b@local
    
scenario_2:
  seq2seq_model: llama3_2-1b@local

scenario_3:
  seq2seq_model: llama-3_1-70B@ovh
  text_embedding_model: nomic-embed-text_faiss@local

scenario_4:
  seq2seq_model: llama-3_1-70B@ovh
  text_embedding_model: nomic-embed-text_faiss@local

scenario_5:
  seq2seq_model: llama-3_1-70B@ovh
  text_embedding_model: nomic-embed-text_faiss@local

scenario_6:
  seq2seq_model: llama-3_1-70B@ovh
  text_embedding_model: nomic-embed-text_faiss@local
