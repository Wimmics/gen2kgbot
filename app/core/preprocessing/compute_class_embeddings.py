"""
This script computes the embeddings of the class descriptions previsouly generated by gen_classes_description.py.

It uses the embedding model defined in the configuration file and saves the embeddings to the local file system.

The output is saved to the directory defined in param "class_embeddings_subdir" in the configuration file.
"""

from argparse import ArgumentParser
import faiss
import os
from tqdm import tqdm
from langchain_community.vectorstores import FAISS
from langchain_community.docstore import InMemoryDocstore
from langchain_chroma import Chroma


def setup_cli():
    parser = ArgumentParser(
        description="Compute the embeddings of the class descriptions previsouly generated."
    )
    parser.add_argument(
        "-m",
        "--model",
        type=str,
        help="Embedding model description in the configuration file. Default: 'nomic-embed-text_faiss@local'",
        default="nomic-embed-text_faiss@local",
    )
    parser.add_argument(
        "-c",
        "--classes",
        type=str,
        help='File with the description of the classes. Must be in directory "{data_directory}/{KG short name}/preprocessing". Default: "classes_with_instances_description.txt"',
        default="classes_with_instances_description.txt",
    )
    return parser.parse_args()


arguments = setup_cli()

# Import the config after initializing the argparse otherwise the one configured in config_manager takes over
import app.core.utils.config_manager as config


def chunks(lst: list, n):
    """Yield successive n-sized chunks from lst."""
    for i in range(0, len(lst), n):
        yield lst[i : i + n]


logger = config.setup_logger(__package__, __file__)


if __name__ == "__main__":
    # Name of the embdingg model as shown in the configuration file
    embed_name = arguments.model

    embed_config = config.get_embedding_model_config_by_name(embed_name)

    # Load the classes description
    classes_description_file = os.path.join(
        config.get_classes_preprocessing_directory(),
        arguments.classes,
    )
    if not os.path.exists(classes_description_file):
        logger.error(f"Classes description file not found: {classes_description_file}")
        raise Exception(
            f"Classes description file not found: {classes_description_file}"
        )

    logger.info(f"Loading classes description from {classes_description_file}")
    f = open(classes_description_file, "r", encoding="utf8")
    classes_description = [line.strip() for line in f.readlines()]
    f.close()

    # Create the vector store
    vector_db_name = embed_config["vector_db"]
    embeddings_dir = f"{config.get_embeddings_directory(vector_db_name)}/{config.get_class_embeddings_subdir()}"
    embedding_model = config.get_embedding_model_by_embed_name(embed_name)
    if vector_db_name == "faiss":
        vectorstore = FAISS(
            embedding_function=embedding_model,
            docstore=InMemoryDocstore(),
            index=faiss.IndexFlatL2(len(embedding_model.embed_query("hello world"))),
            index_to_docstore_id={},
        )
    elif vector_db_name == "chroma":
        vectorstore = Chroma(
            persist_directory=embeddings_dir, embedding_function=embedding_model
        )
    else:
        logger.error(f"Unsupported type of vector DB: {vector_db_name}")
        raise Exception(f"Unsupported type of vector DB: {vector_db_name}")

    # Compute the embeddings and add them the vector store
    with tqdm(total=len(classes_description), desc="Ingesting documents") as pbar:
        for sublist in chunks(classes_description, 10):
            vectorstore.add_texts(sublist)
            pbar.update(len(sublist))

    # Saving the embeddings
    logger.info(f"Saving embeddings to directory: {embeddings_dir}")
    vectorstore.save_local(embeddings_dir)
